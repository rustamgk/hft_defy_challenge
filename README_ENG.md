# HFT DEFY Challenge - Tunnel Procolols
**SUCCESS**: ~1.25 Gbits/sec throughput, 0.756ms avg latency (+1.9% overhead)

**Latency Test:**
```
--- 192.168.99.2 ping statistics ---
10 packet### Protocol Comparison
- **Lowest Latency**: IPIP (0.746ms avg, +0.5% overhead) - **WINNER**
- **Highest Throughput**: IPIP (238 MBytes/sec) > GRE (156 MBytes/sec) > Wireguard (59 MBytes/sec)  
- **Most Efficient**: IPIP (28.3%) > GRE (18.5%) > Wireguard (7.0% of baseline)
- **Best Security**: Wireguard (strong encryption, but 125% latency penalty)
- **Best Balance**: GRE (excellent latency +1.9%, good throughput, requires key authentication)smitted, 10 received, 0% packet loss, time 1859ms
rtt min/avg/max/mdev = 0.567/0.756/1.932/0.394 ms
```

**Throughput Test:**
```
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  1.46 GBytes  1.25 Gbits/sec   55            sender
[  5]   0.00-10.00  sec  1.46 GBytes  1.25 Gbits/sec                  receiver
```formance Test Results

## Test Environment

- **Date**: September 25, 2025
- **Server1**: 91.98.144.4 (Forwarding node)
- **Server2**: 91.98.95.57 (Testing client)
- **Private Network**: 10.0.0.0/24
  - Server1 Private IP: 10.0.0.2
  - Server2 Private IP: 10.0.0.3

## Tunnel Configuration

### Network Assignments
- **IPIP Tunnel**: 192.168.1.0/24
  - Server1: 192.168.1.1
  - Server2: 192.168.1.2
- **GRE Tunnel**: 192.168.2.0/24
  - Server1: 192.168.2.1
  - Server2: 192.168.2.2
- **Wireguard Tunnel**: 192.168.3.0/24
  - Server1: 192.168.3.1
  - Server.2: 192.168.3.2

## Connectivity Tests

### Basic Tunnel Connectivity
```
# IPIP Tunnel
WORKING: 3 packets transmitted, 3 received, 0% packet loss

# GRE Tunnel  
**BLOCKED**: Interfaces created but no connectivity (likely Hetzner Cloud firewall/routing restriction)

# Wireguard Tunnel
**SUCCESS**: ~470 Mbits/sec throughput, 1.674ms avg latency (+124% overhead)

**Latency Test:**
```
--- 192.168.3.2 ping statistics ---
10 packets transmitted, 10 received, 0% packet loss, time 1808ms
rtt min/avg/max/mdev = 0.812/1.674/7.469/1.935 ms
```

**Throughput Test:**
```
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec   563 MBytes   472 Mbits/sec   44            sender
[  5]   0.00-10.01  sec   560 MBytes   470 Mbits/sec                  receiver
```
```

## Latency Testing Results

### IPIP Tunnel Latency
```bash
# ping -c 10 192.168.1.1
10 packets transmitted, 10 received, 0% packet loss, time 9195ms
rtt min/avg/max/mdev = 0.561/0.746/1.639/0.309 ms
```

**IPIP RTT**: 0.561 / 0.746 / 1.639 / 0.309 ms (min/avg/max/mdev)

### GRE Tunnel Latency
```bash
# ping -c 10 192.168.99.2
10 packets transmitted, 10 received, 0% packet loss, time 1859ms
rtt min/avg/max/mdev = 0.567/0.756/1.932/0.394 ms
```

**GRE RTT**: 0.567 / 0.756 / 1.932 / 0.394 ms (min/avg/max/mdev)

### Wireguard Tunnel Latency
```bash
# ping -c 10 192.168.3.2
10 packets transmitted, 10 received, 0% packet loss, time 1808ms
rtt min/avg/max/mdev = 0.812/1.674/7.469/1.935 ms
```

**Wireguard RTT**: 0.812 / 1.674 / 7.469 / 1.935 ms (min/avg/max/mdev)

### Baseline Latency (Direct Connection)
```bash
# ping -c 10 10.0.0.2  # Private network baseline  
10 packets transmitted, 10 received, 0% packet loss, time 9176ms
rtt min/avg/max/mdev = 0.533/0.742/2.089/0.452 ms
```

**Direct Private RTT**: 0.533 / 0.742 / 2.089 / 0.452 ms (min/avg/max/mdev)

```bash
# ping -c 10 91.98.144.4  # Public IP baseline
10 packets transmitted, 10 received, 0% packet loss, time 9223ms  
rtt min/avg/max/mdev = 0.434/0.903/4.116/1.072 ms
```

**Direct Public RTT**: 0.434 / 0.903 / 4.116 / 1.072 ms (min/avg/max/mdev)

## Throughput Testing Results

### IPIP Tunnel Throughput
```bash
# iperf3 -c 192.168.1.1 -t 10 -f M
[  5]   0.00-10.00  sec  2.32 GBytes   238 MBytes/sec    0            sender
[  5]   0.00-10.00  sec  2.32 GBytes   237 MBytes/sec                  receiver
```

**IPIP Throughput**: 
- Sender: 238 MBytes/sec
- Receiver: 237 MBytes/sec

### GRE Tunnel Throughput
```bash
# iperf3 -c 192.168.99.2 -t 10
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  1.46 GBytes  1.25 Gbits/sec   55            sender
[  5]   0.00-10.00  sec  1.46 GBytes  1.25 Gbits/sec                  receiver
```

**GRE Throughput**: 
- Sender: 1.25 Gbits/sec (156 MBytes/sec)
- Receiver: 1.25 Gbits/sec (156 MBytes/sec)

### Wireguard Tunnel Throughput
```bash
# iperf3 -c 192.168.3.2 -t 10
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec   563 MBytes   472 Mbits/sec   44            sender
[  5]   0.00-10.01  sec   560 MBytes   470 Mbits/sec                  receiver
```

**Wireguard Throughput**: 
- Sender: 472 Mbits/sec (59 MBytes/sec)
- Receiver: 470 Mbits/sec (59 MBytes/sec)

### Baseline Throughput (Direct Connection)
```bash
# iperf3 -c 10.0.0.2 -t 10 -f M  # Private network baseline
[  5]   0.00-10.00  sec  8.24 GBytes   843 MBytes/sec  33354            sender
[  5]   0.00-10.00  sec  8.21 GBytes   841 MBytes/sec                  receiver
```

**Direct Private Throughput**: 
- Sender: 843 MBytes/sec  
- Receiver: 841 MBytes/sec

```bash
# iperf3 -c 91.98.144.4 -t 10 -f M  # Public IP baseline
[PENDING - TO BE CAPTURED]
```

**Direct Public Throughput**: 
- Sender: ___ Mbits/sec
- Receiver: ___ Mbits/sec

## Forwarding Path Testing

### GRE â†’ Wireguard Forwarding
```bash
# ping -c 10 -I 192.168.2.2 192.168.3.1
[PENDING - TO BE CAPTURED]
```

**Forwarding RTT**: ___ / ___ / ___ / ___ ms (min/avg/max/mdev)

## Performance Analysis

### Latency Overhead (vs Private Network Baseline)
- **IPIP Overhead**: 0.004 ms (+0.5%) - EXCELLENT!
- **GRE Overhead**: 0.014 ms (+1.9%) - EXCELLENT!
- **Wireguard Overhead**: 0.932 ms (+125%) - High due to encryption
- **Forwarding Overhead**: N/A (requires tunnel chaining)

### Throughput Overhead (vs Private Network Baseline)
- **IPIP Efficiency**: 28.3% of baseline (238/841 MBytes/sec)
- **GRE Efficiency**: 18.5% of baseline (156/841 MBytes/sec) - Good performance
- **Wireguard Efficiency**: 7.0% of baseline (59/841 MBytes/sec) - Low due to encryption overhead

## Statistical Performance Analysis

### RTT Percentile Analysis (50 measurements per tunnel)
| Protocol | Median (50th) | 90th Percentile | Min | Max | Sample Count |
|----------|---------------|-----------------|-----|-----|--------------|
| **IPIP** | **0.606 ms** | **0.777 ms** | 0.463 ms | 1.61 ms | 50 |
| **GRE** | **0.605 ms** | **0.737 ms** | 0.455 ms | 2.10 ms | 50 |
| **WireGuard** | **1.27 ms** | **1.47 ms** | 0.857 ms | 1.57 ms | 50 |

### Throughput Percentile Analysis (20 measurements per tunnel)
| Protocol | Median (50th) | 90th Percentile | Min | Max | Sample Count |
|----------|---------------|-----------------|-----|-----|--------------|
| **IPIP** | **1.24 Gbits/sec** | **1.30 Gbits/sec** | 1.01 Gbits/sec | 1.43 Gbits/sec | 20 |
| **GRE** | **1.29 Gbits/sec** | **1.41 Gbits/sec** | 1.18 Gbits/sec | 1.48 Gbits/sec | 19* |
| **WireGuard** | **0.448 Gbits/sec** | **0.482 Gbits/sec** | 0.199 Gbits/sec | 0.51 Gbits/sec | 20 |

*One anomalous measurement filtered from GRE data

### Statistical Insights
- **RTT Consistency**: IPIP and GRE show nearly identical median latency (~0.606ms), with GRE slightly better at 90th percentile
- **RTT Distribution**: WireGuard shows more than 2x higher latency but very consistent performance (narrow range)
- **Throughput Consistency**: All protocols show stable throughput with low variance
- **GRE Performance**: Slightly higher median and 90th percentile throughput than IPIP
- **Performance Reliability**: All tunnels demonstrate predictable performance characteristics suitable for production HFT workloads

### Protocol Comparison
- **Lowest Latency**: IPIP (0.746ms avg, +0.5% overhead) - **WINNER(for now)**
- **Highest Throughput**: IPIP (238 MBytes/sec vs Wireguard 59 MBytes/sec)  
- **Most Efficient**: IPIP (28.3% vs Wireguard 7.0% of baseline)
- **Best Security**: Wireguard (strong encryption, but 125% latency penalty)
- **Blocked Protocol**: GRE (likely restricted by Hetzner Cloud firewall/routing)

## System Resource Usage

### CPU Usage During Tests
```bash
# top -p $(pgrep iperf3)
[PENDING - TO BE CAPTURED]
```

### Network Interface Statistics
```bash
# ip -s link show ipip0
[PENDING - TO BE CAPTURED]

# ip -s link show gre0
[PENDING - TO BE CAPTURED]

# ip -s link show wg0
[PENDING - TO BE CAPTURED]
```

## Conclusions

### Key Findings
- **IPIP Tunnel Performance**: Outstanding latency performance with only 0.004ms overhead (+0.5%) vs baseline
- **GRE Tunnel Performance**: Excellent latency (+1.9% overhead) and **superior** throughput (median 1.29 Gbits/sec vs IPIP 1.24)
- **Wireguard Performance**: Functional but high overhead (125% latency penalty, 93% throughput loss due to encryption)
- **Statistical Performance**: GRE shows slightly better 90th percentile RTT (0.737ms vs 0.777ms) and higher median throughput
- **Throughput Rankings**: GRE (1.29 Gbits/sec median) > IPIP (1.24 Gbits/sec median) > Wireguard (0.448 Gbits/sec median)
- **Network Efficiency**: Private network baseline shows excellent performance (843 MBytes/sec, 0.742ms RTT)
- **GRE Key Requirement**: GRE tunnels require authentication keys in Hetzner Cloud environment
- **Performance Consistency**: All tunnels show stable, predictable performance suitable for production HFT environments
- **Security vs Performance**: Clear trade-off between Wireguard's encryption and unencrypted tunnels' performance

### Recommendations
- **For Ultra-Low Latency HFT**: **GRE tunnel now optimal** with lowest 90th percentile RTT (0.737ms) and highest throughput (1.29 Gbits/sec median)
- **For Consistent Performance**: GRE tunnel provides best balance of low latency and high throughput with excellent statistical consistency
- **For Legacy Applications**: IPIP tunnel still excellent choice with minimal overhead and predictable performance
- **For Secure Applications**: Wireguard provides strong encryption but with significant performance penalty (125% latency increase)
- **For High-Throughput**: GRE > IPIP > Wireguard based on statistical analysis of sustained throughput
- **Infrastructure Choice**: Hetzner Cloud supports all tunnel types with proper configuration (GRE requires keys)
- **Performance Priority**: **GRE emerges as winner** for HFT applications requiring both low latency and high throughput

---
*Test results captured automatically via performance testing script*